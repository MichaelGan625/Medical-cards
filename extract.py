import zipfile
import sqlite3
import json
import os
import shutil
import tempfile
from bs4 import BeautifulSoup
import re
# --- é…ç½®åŒºåŸŸ ---
ANKI_FILE_PATH = 'AnKing V11 updated.apkg'  # è¯·ä¿®æ”¹ä¸ºä½ çš„æ–‡ä»¶å
OUTPUT_FILE = 'sample.json'
SAMPLE_SIZE = 10000000  # å¯¼å¸ˆè¦æ±‚å…ˆåšå°æ‰¹é‡æµ‹è¯•
FILTER_TAG = "Step2"  # è¿‡æ»¤å…³é”®è¯ï¼Œå¦‚æœä¸å¡«åˆ™å¯¼å‡ºæ‰€æœ‰


def deep_clean_text(text):
    if not text:
        return ""

    # 1. å»é™¤ HTML æ ‡ç­¾ (ä¿ç•™ç©ºæ ¼ä»¥å…å•è¯ç²˜è¿)
    soup = BeautifulSoup(text, "html.parser")
    text = soup.get_text(separator=" ", strip=True)

    # 2. å»é™¤éŸ³é¢‘/å›¾ç‰‡å¼•ç”¨ [sound:...] æˆ– [img:...]
    text = re.sub(r'\[.*?\]', '', text)

    # 3. æ¸…æ´—å¡«ç©ºé¢˜è¯­æ³• {{c1::Answer}} -> Answer
    # é€»è¾‘ï¼šæ‰¾åˆ° {{cæ•°å­—::å†…å®¹}}ï¼Œåªä¿ç•™å†…å®¹
    text = re.sub(r'\{\{c\d+::(.*?)(::.*?)?\}\}', r'\1', text)

    # 4. å»é™¤å¤šä½™çš„ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()

    return text


def extract_anki_data(apkg_path, output_path, limit=100):
    print(f"ğŸ”„ å¼€å§‹å¤„ç†: {apkg_path}...")

    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤¹æ¥è§£å‹æ•°æ®åº“
    with tempfile.TemporaryDirectory() as temp_dir:
        try:
            # 1. è§£å‹ .apkg (æœ¬è´¨æ˜¯ zip)
            with zipfile.ZipFile(apkg_path, 'r') as z:
                z.extractall(temp_dir)

            db_path = os.path.join(temp_dir, 'collection.anki2')

            if not os.path.exists(db_path):
                print("âŒ é”™è¯¯: æ— æ³•åœ¨åŒ…ä¸­æ‰¾åˆ°æ•°æ®åº“æ–‡ä»¶ collection.anki2")
                return

            # 2. è¿æ¥ SQLite æ•°æ®åº“
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()

            # 3. æŸ¥è¯¢ notes è¡¨ (flds åŒ…å«å†…å®¹, tags åŒ…å«æ ‡ç­¾)
            # Anki çš„å­—æ®µæ˜¯ç”¨ \x1f (Unit Separator) åˆ†éš”çš„å­—ç¬¦ä¸²
            cursor.execute("SELECT flds, tags FROM notes")

            extracted_data = []
            count = 0
            total_scanned = 0

            print("ğŸ” æ­£åœ¨æ‰«æå¹¶æ¸…æ´—æ•°æ®...")

            for row in cursor:
                flds_str, tags_str = row
                total_scanned += 1

                # æ£€æŸ¥æ ‡ç­¾æ˜¯å¦åŒ…å« "Step 2" (å¦‚æœè®¾ç½®äº†è¿‡æ»¤)
                if FILTER_TAG and (FILTER_TAG.lower() not in tags_str.lower()):
                    continue

                # 4. æ•°æ®æ¸…æ´—ä¸åˆ†å‰²
                fields = flds_str.split('\x1f')

                # é€šå¸¸ Field 0 æ˜¯æ­£é¢(é—®é¢˜), Field 1 æ˜¯èƒŒé¢(ç­”æ¡ˆ)ï¼Œä½†ä¹Ÿå¯èƒ½æœ‰æ›´å¤šå­—æ®µ
                # æˆ‘ä»¬æŠŠæ‰€æœ‰å­—æ®µæ¸…æ´—åå­˜å…¥åˆ—è¡¨
                cleaned_fields = [deep_clean_text(f) for f in fields]

                card_obj = {
                    "id": total_scanned,
                    "tags": tags_str.strip(),
                    "front": cleaned_fields[0] if len(cleaned_fields) > 0 else "",
                    "back": cleaned_fields[1] if len(cleaned_fields) > 1 else "",
                    "extra_fields": cleaned_fields[2:]  # å¦‚æœæœ‰é¢å¤–å­—æ®µ
                }

                extracted_data.append(card_obj)
                count += 1

                # è¾¾åˆ°é™åˆ¶æ•°é‡åœæ­¢
                if count >= limit:
                    break

            conn.close()

            # 5. ä¿å­˜ä¸º JSON
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(extracted_data, f, indent=4, ensure_ascii=False)

            print(f"âœ… æˆåŠŸ! å·²æå– {count} æ¡åŒ…å« '{FILTER_TAG}' çš„æ•°æ®ã€‚")
            print(f"ğŸ“‚ ç»“æœå·²ä¿å­˜è‡³: {output_path}")
            print(f"ğŸ“Š æ‰«æè¿›åº¦: æ‰«æäº†å‰ {total_scanned} æ¡æ•°æ®æ‰¾åˆ°ç›®æ ‡æ ·æœ¬ã€‚")

        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")


# --- è¿è¡Œè„šæœ¬ ---
if __name__ == "__main__":
    # ç¡®ä¿è¿™é‡Œçš„æ–‡ä»¶åæ˜¯æ­£ç¡®çš„
    if not os.path.exists(ANKI_FILE_PATH):
        print(f"âš ï¸ æ‰¾ä¸åˆ°æ–‡ä»¶: {ANKI_FILE_PATH}ï¼Œè¯·ä¿®æ”¹è„šæœ¬ä¸­çš„æ–‡ä»¶åã€‚")
    else:
        extract_anki_data(ANKI_FILE_PATH, OUTPUT_FILE, limit=SAMPLE_SIZE)